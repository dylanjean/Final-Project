{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "import seaborn as sns\n",
    "from keras.layers import Input,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Last</th>\n",
       "      <th>Bid</th>\n",
       "      <th>Ask</th>\n",
       "      <th>Volume</th>\n",
       "      <th>VWAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-15</td>\n",
       "      <td>515.00</td>\n",
       "      <td>453.16</td>\n",
       "      <td>499.01</td>\n",
       "      <td>500.01</td>\n",
       "      <td>505.04</td>\n",
       "      <td>28535.844106</td>\n",
       "      <td>491.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-16</td>\n",
       "      <td>548.00</td>\n",
       "      <td>494.02</td>\n",
       "      <td>534.00</td>\n",
       "      <td>535.01</td>\n",
       "      <td>536.00</td>\n",
       "      <td>31159.941300</td>\n",
       "      <td>520.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-17</td>\n",
       "      <td>537.24</td>\n",
       "      <td>481.63</td>\n",
       "      <td>506.52</td>\n",
       "      <td>504.70</td>\n",
       "      <td>505.38</td>\n",
       "      <td>21126.375080</td>\n",
       "      <td>504.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2014-04-18</td>\n",
       "      <td>508.43</td>\n",
       "      <td>470.00</td>\n",
       "      <td>487.00</td>\n",
       "      <td>484.14</td>\n",
       "      <td>487.00</td>\n",
       "      <td>11879.484756</td>\n",
       "      <td>485.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2014-04-19</td>\n",
       "      <td>507.43</td>\n",
       "      <td>472.81</td>\n",
       "      <td>504.74</td>\n",
       "      <td>504.74</td>\n",
       "      <td>505.00</td>\n",
       "      <td>10262.195861</td>\n",
       "      <td>492.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>2378</td>\n",
       "      <td>2020-11-14</td>\n",
       "      <td>16494.52</td>\n",
       "      <td>15970.33</td>\n",
       "      <td>16335.58</td>\n",
       "      <td>16335.58</td>\n",
       "      <td>16339.27</td>\n",
       "      <td>7842.488826</td>\n",
       "      <td>16279.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>2379</td>\n",
       "      <td>2020-11-15</td>\n",
       "      <td>16341.89</td>\n",
       "      <td>15715.10</td>\n",
       "      <td>16086.34</td>\n",
       "      <td>16087.77</td>\n",
       "      <td>16094.81</td>\n",
       "      <td>5046.326705</td>\n",
       "      <td>15982.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>2380</td>\n",
       "      <td>2020-11-16</td>\n",
       "      <td>16170.00</td>\n",
       "      <td>15786.46</td>\n",
       "      <td>15975.49</td>\n",
       "      <td>15969.41</td>\n",
       "      <td>15973.22</td>\n",
       "      <td>3226.276565</td>\n",
       "      <td>15979.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>2381</td>\n",
       "      <td>2020-11-17</td>\n",
       "      <td>16894.93</td>\n",
       "      <td>15875.50</td>\n",
       "      <td>16724.62</td>\n",
       "      <td>16719.84</td>\n",
       "      <td>16729.63</td>\n",
       "      <td>7511.143605</td>\n",
       "      <td>16409.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>2382</td>\n",
       "      <td>2020-11-18</td>\n",
       "      <td>17868.00</td>\n",
       "      <td>16570.00</td>\n",
       "      <td>17681.77</td>\n",
       "      <td>17670.67</td>\n",
       "      <td>17680.50</td>\n",
       "      <td>9824.899652</td>\n",
       "      <td>17260.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2383 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index        Date      High       Low      Last       Bid       Ask  \\\n",
       "0         0  2014-04-15    515.00    453.16    499.01    500.01    505.04   \n",
       "1         1  2014-04-16    548.00    494.02    534.00    535.01    536.00   \n",
       "2         2  2014-04-17    537.24    481.63    506.52    504.70    505.38   \n",
       "3         3  2014-04-18    508.43    470.00    487.00    484.14    487.00   \n",
       "4         4  2014-04-19    507.43    472.81    504.74    504.74    505.00   \n",
       "...     ...         ...       ...       ...       ...       ...       ...   \n",
       "2378   2378  2020-11-14  16494.52  15970.33  16335.58  16335.58  16339.27   \n",
       "2379   2379  2020-11-15  16341.89  15715.10  16086.34  16087.77  16094.81   \n",
       "2380   2380  2020-11-16  16170.00  15786.46  15975.49  15969.41  15973.22   \n",
       "2381   2381  2020-11-17  16894.93  15875.50  16724.62  16719.84  16729.63   \n",
       "2382   2382  2020-11-18  17868.00  16570.00  17681.77  17670.67  17680.50   \n",
       "\n",
       "            Volume      VWAP  \n",
       "0     28535.844106    491.41  \n",
       "1     31159.941300    520.21  \n",
       "2     21126.375080    504.83  \n",
       "3     11879.484756    485.72  \n",
       "4     10262.195861    492.22  \n",
       "...            ...       ...  \n",
       "2378   7842.488826  16279.18  \n",
       "2379   5046.326705  15982.98  \n",
       "2380   3226.276565  15979.39  \n",
       "2381   7511.143605  16409.99  \n",
       "2382   9824.899652  17260.21  \n",
       "\n",
       "[2383 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('bitcoin_data.csv')\n",
    "data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns='Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Last</th>\n",
       "      <th>Bid</th>\n",
       "      <th>Ask</th>\n",
       "      <th>Volume</th>\n",
       "      <th>VWAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>13650.00</td>\n",
       "      <td>12993.00</td>\n",
       "      <td>13411.86</td>\n",
       "      <td>13413.35</td>\n",
       "      <td>13416.39</td>\n",
       "      <td>8948.197381</td>\n",
       "      <td>13318.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>13675.56</td>\n",
       "      <td>13129.26</td>\n",
       "      <td>13577.60</td>\n",
       "      <td>13572.18</td>\n",
       "      <td>13581.82</td>\n",
       "      <td>8980.671631</td>\n",
       "      <td>13404.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>14100.00</td>\n",
       "      <td>13420.97</td>\n",
       "      <td>13879.14</td>\n",
       "      <td>13878.39</td>\n",
       "      <td>13887.22</td>\n",
       "      <td>6394.173016</td>\n",
       "      <td>13762.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>13907.47</td>\n",
       "      <td>13629.31</td>\n",
       "      <td>13711.21</td>\n",
       "      <td>13700.59</td>\n",
       "      <td>13711.21</td>\n",
       "      <td>2465.795017</td>\n",
       "      <td>13766.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>13842.50</td>\n",
       "      <td>13220.00</td>\n",
       "      <td>13563.72</td>\n",
       "      <td>13570.69</td>\n",
       "      <td>13578.98</td>\n",
       "      <td>7062.704712</td>\n",
       "      <td>13545.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>14083.76</td>\n",
       "      <td>13287.70</td>\n",
       "      <td>14041.58</td>\n",
       "      <td>14030.89</td>\n",
       "      <td>14041.58</td>\n",
       "      <td>7226.516356</td>\n",
       "      <td>13633.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>14277.50</td>\n",
       "      <td>13520.87</td>\n",
       "      <td>14160.59</td>\n",
       "      <td>14162.68</td>\n",
       "      <td>14171.73</td>\n",
       "      <td>10925.676805</td>\n",
       "      <td>13908.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>15770.58</td>\n",
       "      <td>14100.00</td>\n",
       "      <td>15605.04</td>\n",
       "      <td>15603.49</td>\n",
       "      <td>15608.32</td>\n",
       "      <td>18422.631860</td>\n",
       "      <td>14937.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>15968.98</td>\n",
       "      <td>15196.01</td>\n",
       "      <td>15598.09</td>\n",
       "      <td>15598.10</td>\n",
       "      <td>15603.23</td>\n",
       "      <td>13479.467068</td>\n",
       "      <td>15570.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>15778.60</td>\n",
       "      <td>14351.00</td>\n",
       "      <td>14838.97</td>\n",
       "      <td>14832.01</td>\n",
       "      <td>14842.70</td>\n",
       "      <td>10933.924192</td>\n",
       "      <td>15076.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>15664.90</td>\n",
       "      <td>14727.19</td>\n",
       "      <td>15489.15</td>\n",
       "      <td>15480.45</td>\n",
       "      <td>15490.09</td>\n",
       "      <td>5046.499164</td>\n",
       "      <td>15255.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>15854.48</td>\n",
       "      <td>14824.66</td>\n",
       "      <td>15332.04</td>\n",
       "      <td>15335.50</td>\n",
       "      <td>15348.95</td>\n",
       "      <td>14466.079537</td>\n",
       "      <td>15364.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>15482.76</td>\n",
       "      <td>15092.47</td>\n",
       "      <td>15313.65</td>\n",
       "      <td>15313.87</td>\n",
       "      <td>15317.81</td>\n",
       "      <td>8966.481714</td>\n",
       "      <td>15325.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>15991.01</td>\n",
       "      <td>15290.85</td>\n",
       "      <td>15702.00</td>\n",
       "      <td>15696.48</td>\n",
       "      <td>15703.40</td>\n",
       "      <td>10014.264272</td>\n",
       "      <td>15652.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>16369.99</td>\n",
       "      <td>15481.00</td>\n",
       "      <td>16300.00</td>\n",
       "      <td>16297.91</td>\n",
       "      <td>16300.00</td>\n",
       "      <td>16261.206579</td>\n",
       "      <td>15939.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>16494.52</td>\n",
       "      <td>15970.33</td>\n",
       "      <td>16335.58</td>\n",
       "      <td>16335.58</td>\n",
       "      <td>16339.27</td>\n",
       "      <td>7842.488826</td>\n",
       "      <td>16279.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>16341.89</td>\n",
       "      <td>15715.10</td>\n",
       "      <td>16086.34</td>\n",
       "      <td>16087.77</td>\n",
       "      <td>16094.81</td>\n",
       "      <td>5046.326705</td>\n",
       "      <td>15982.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>16170.00</td>\n",
       "      <td>15786.46</td>\n",
       "      <td>15975.49</td>\n",
       "      <td>15969.41</td>\n",
       "      <td>15973.22</td>\n",
       "      <td>3226.276565</td>\n",
       "      <td>15979.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>16894.93</td>\n",
       "      <td>15875.50</td>\n",
       "      <td>16724.62</td>\n",
       "      <td>16719.84</td>\n",
       "      <td>16729.63</td>\n",
       "      <td>7511.143605</td>\n",
       "      <td>16409.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>17868.00</td>\n",
       "      <td>16570.00</td>\n",
       "      <td>17681.77</td>\n",
       "      <td>17670.67</td>\n",
       "      <td>17680.50</td>\n",
       "      <td>9824.899652</td>\n",
       "      <td>17260.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          High       Low      Last       Bid       Ask        Volume      VWAP\n",
       "2363  13650.00  12993.00  13411.86  13413.35  13416.39   8948.197381  13318.04\n",
       "2364  13675.56  13129.26  13577.60  13572.18  13581.82   8980.671631  13404.59\n",
       "2365  14100.00  13420.97  13879.14  13878.39  13887.22   6394.173016  13762.10\n",
       "2366  13907.47  13629.31  13711.21  13700.59  13711.21   2465.795017  13766.72\n",
       "2367  13842.50  13220.00  13563.72  13570.69  13578.98   7062.704712  13545.15\n",
       "2368  14083.76  13287.70  14041.58  14030.89  14041.58   7226.516356  13633.01\n",
       "2369  14277.50  13520.87  14160.59  14162.68  14171.73  10925.676805  13908.16\n",
       "2370  15770.58  14100.00  15605.04  15603.49  15608.32  18422.631860  14937.25\n",
       "2371  15968.98  15196.01  15598.09  15598.10  15603.23  13479.467068  15570.55\n",
       "2372  15778.60  14351.00  14838.97  14832.01  14842.70  10933.924192  15076.99\n",
       "2373  15664.90  14727.19  15489.15  15480.45  15490.09   5046.499164  15255.08\n",
       "2374  15854.48  14824.66  15332.04  15335.50  15348.95  14466.079537  15364.34\n",
       "2375  15482.76  15092.47  15313.65  15313.87  15317.81   8966.481714  15325.34\n",
       "2376  15991.01  15290.85  15702.00  15696.48  15703.40  10014.264272  15652.52\n",
       "2377  16369.99  15481.00  16300.00  16297.91  16300.00  16261.206579  15939.46\n",
       "2378  16494.52  15970.33  16335.58  16335.58  16339.27   7842.488826  16279.18\n",
       "2379  16341.89  15715.10  16086.34  16087.77  16094.81   5046.326705  15982.98\n",
       "2380  16170.00  15786.46  15975.49  15969.41  15973.22   3226.276565  15979.39\n",
       "2381  16894.93  15875.50  16724.62  16719.84  16729.63   7511.143605  16409.99\n",
       "2382  17868.00  16570.00  17681.77  17670.67  17680.50   9824.899652  17260.21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = data.tail(20)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "High = test.High.values\n",
    "Low = test.Low.values\n",
    "Volume = test.Volume.values\n",
    "Last = test.Last.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13650.  , 12993.  ],\n",
       "       [13675.56, 13129.26],\n",
       "       [14100.  , 13420.97],\n",
       "       [13907.47, 13629.31],\n",
       "       [13842.5 , 13220.  ],\n",
       "       [14083.76, 13287.7 ],\n",
       "       [14277.5 , 13520.87],\n",
       "       [15770.58, 14100.  ],\n",
       "       [15968.98, 15196.01],\n",
       "       [15778.6 , 14351.  ],\n",
       "       [15664.9 , 14727.19],\n",
       "       [15854.48, 14824.66],\n",
       "       [15482.76, 15092.47],\n",
       "       [15991.01, 15290.85],\n",
       "       [16369.99, 15481.  ],\n",
       "       [16494.52, 15970.33],\n",
       "       [16341.89, 15715.1 ],\n",
       "       [16170.  , 15786.46],\n",
       "       [16894.93, 15875.5 ],\n",
       "       [17868.  , 16570.  ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(High)):\n",
    "    row = []\n",
    "    yrow = []\n",
    "    row.append(High[i])\n",
    "    row.append(Low[i])\n",
    "#     row.append(Volume[i])\n",
    "    yrow.append(Last[i])\n",
    "    X.append(row)\n",
    "    y.append(yrow)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "horsepower = np.array(X_train)\n",
    "horsepower2 = np.array(X_train)\n",
    "horsepower_normalizer = preprocessing.Normalization(input_shape=[1,])\n",
    "horsepower_normalizer.adapt(horsepower)\n",
    "horsepower_normalizer2 = preprocessing.Normalization(input_shape=[1,])\n",
    "horsepower_normalizer2.adapt(horsepower2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input_layer = Input(shape=(X.shape[1],))\n",
    "# dense_layer_1 = Dense(100, input_shape=(X.shape[1],), activation='relu')\n",
    "# dense_layer_2 = Dense(50, activation='relu')(dense_layer_1)\n",
    "# dense_layer_3 = Dense(25, activation='relu')(dense_layer_2)\n",
    "# output = Dense(1)\n",
    "\n",
    "# model = Model([\n",
    "#     Dense(100, input_dim=X.shape[1], activation='relu'),\n",
    "#     Dense(50, activation='relu'),\n",
    "#     Dense(25, activation='relu'),\n",
    "#     Dense(units=1, activation='softmax')\n",
    "# ])\n",
    "# model.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "\n",
    "horsepower_model = tf.keras.Sequential([\n",
    "#     horsepower_normalizer,\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(units=1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1282.6438],\n",
       "       [1041.2924],\n",
       "       [1356.0498],\n",
       "       [1196.2477]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horsepower_model.predict(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "horsepower_model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mean_absolute_error' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12 samples, validate on 4 samples\n",
      "Epoch 1/100\n",
      "12/12 - 0s - loss: 13758.8545 - val_loss: 13175.4580\n",
      "Epoch 2/100\n",
      "12/12 - 0s - loss: 13399.4209 - val_loss: 12873.2715\n",
      "Epoch 3/100\n",
      "12/12 - 0s - loss: 13089.2529 - val_loss: 12554.0566\n",
      "Epoch 4/100\n",
      "12/12 - 0s - loss: 12761.6064 - val_loss: 12256.8848\n",
      "Epoch 5/100\n",
      "12/12 - 0s - loss: 12456.5859 - val_loss: 11913.8906\n",
      "Epoch 6/100\n",
      "12/12 - 0s - loss: 12104.5342 - val_loss: 11551.9629\n",
      "Epoch 7/100\n",
      "12/12 - 0s - loss: 11733.0439 - val_loss: 11167.6270\n",
      "Epoch 8/100\n",
      "12/12 - 0s - loss: 11338.5576 - val_loss: 10753.7363\n",
      "Epoch 9/100\n",
      "12/12 - 0s - loss: 10913.7354 - val_loss: 10336.1426\n",
      "Epoch 10/100\n",
      "12/12 - 0s - loss: 10485.1113 - val_loss: 9887.8926\n",
      "Epoch 11/100\n",
      "12/12 - 0s - loss: 10025.0225 - val_loss: 9404.9199\n",
      "Epoch 12/100\n",
      "12/12 - 0s - loss: 9529.2920 - val_loss: 8883.4863\n",
      "Epoch 13/100\n",
      "12/12 - 0s - loss: 8994.0859 - val_loss: 8320.7109\n",
      "Epoch 14/100\n",
      "12/12 - 0s - loss: 8416.4482 - val_loss: 7713.5630\n",
      "Epoch 15/100\n",
      "12/12 - 0s - loss: 7793.2632 - val_loss: 7058.6045\n",
      "Epoch 16/100\n",
      "12/12 - 0s - loss: 7121.0054 - val_loss: 6355.7349\n",
      "Epoch 17/100\n",
      "12/12 - 0s - loss: 6399.5703 - val_loss: 5616.1572\n",
      "Epoch 18/100\n",
      "12/12 - 0s - loss: 5640.4590 - val_loss: 4821.3828\n",
      "Epoch 19/100\n",
      "12/12 - 0s - loss: 4824.6929 - val_loss: 3968.9404\n",
      "Epoch 20/100\n",
      "12/12 - 0s - loss: 3949.7346 - val_loss: 3046.0728\n",
      "Epoch 21/100\n",
      "12/12 - 0s - loss: 3002.4919 - val_loss: 2058.4600\n",
      "Epoch 22/100\n",
      "12/12 - 0s - loss: 1988.7935 - val_loss: 993.4945\n",
      "Epoch 23/100\n",
      "12/12 - 0s - loss: 929.5952 - val_loss: 231.2050\n",
      "Epoch 24/100\n",
      "12/12 - 0s - loss: 447.4014 - val_loss: 1205.3605\n",
      "Epoch 25/100\n",
      "12/12 - 0s - loss: 1361.2328 - val_loss: 1911.4139\n",
      "Epoch 26/100\n",
      "12/12 - 0s - loss: 2085.9355 - val_loss: 2302.8428\n",
      "Epoch 27/100\n",
      "12/12 - 0s - loss: 2487.7029 - val_loss: 2425.2749\n",
      "Epoch 28/100\n",
      "12/12 - 0s - loss: 2613.3684 - val_loss: 2324.4858\n",
      "Epoch 29/100\n",
      "12/12 - 0s - loss: 2509.9163 - val_loss: 2043.5480\n",
      "Epoch 30/100\n",
      "12/12 - 0s - loss: 2221.5601 - val_loss: 1621.5233\n",
      "Epoch 31/100\n",
      "12/12 - 0s - loss: 1788.3885 - val_loss: 1092.6129\n",
      "Epoch 32/100\n",
      "12/12 - 0s - loss: 1245.5078 - val_loss: 485.0929\n",
      "Epoch 33/100\n",
      "12/12 - 0s - loss: 667.7200 - val_loss: 267.8944\n",
      "Epoch 34/100\n",
      "12/12 - 0s - loss: 443.1930 - val_loss: 762.4945\n",
      "Epoch 35/100\n",
      "12/12 - 0s - loss: 712.9387 - val_loss: 1116.7372\n",
      "Epoch 36/100\n",
      "12/12 - 0s - loss: 1045.1852 - val_loss: 1297.6774\n",
      "Epoch 37/100\n",
      "12/12 - 0s - loss: 1214.8893 - val_loss: 1350.4696\n",
      "Epoch 38/100\n",
      "12/12 - 0s - loss: 1264.4043 - val_loss: 1312.7780\n",
      "Epoch 39/100\n",
      "12/12 - 0s - loss: 1229.0533 - val_loss: 1173.4281\n",
      "Epoch 40/100\n",
      "12/12 - 0s - loss: 1098.3560 - val_loss: 940.7242\n",
      "Epoch 41/100\n",
      "12/12 - 0s - loss: 880.1019 - val_loss: 620.0753\n",
      "Epoch 42/100\n",
      "12/12 - 0s - loss: 602.4493 - val_loss: 300.0895\n",
      "Epoch 43/100\n",
      "12/12 - 0s - loss: 449.0996 - val_loss: 231.2050\n",
      "Epoch 44/100\n",
      "12/12 - 0s - loss: 439.5448 - val_loss: 415.7999\n",
      "Epoch 45/100\n",
      "12/12 - 0s - loss: 608.4539 - val_loss: 584.3500\n",
      "Epoch 46/100\n",
      "12/12 - 0s - loss: 756.3589 - val_loss: 605.5194\n",
      "Epoch 47/100\n",
      "12/12 - 0s - loss: 776.3066 - val_loss: 495.7025\n",
      "Epoch 48/100\n",
      "12/12 - 0s - loss: 676.7949 - val_loss: 287.7260\n",
      "Epoch 49/100\n",
      "12/12 - 0s - loss: 502.4149 - val_loss: 231.2050\n",
      "Epoch 50/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 295.9108\n",
      "Epoch 51/100\n",
      "12/12 - 0s - loss: 448.3331 - val_loss: 455.7491\n",
      "Epoch 52/100\n",
      "12/12 - 0s - loss: 504.2379 - val_loss: 571.5250\n",
      "Epoch 53/100\n",
      "12/12 - 0s - loss: 573.4324 - val_loss: 602.4921\n",
      "Epoch 54/100\n",
      "12/12 - 0s - loss: 591.9399 - val_loss: 557.2506\n",
      "Epoch 55/100\n",
      "12/12 - 0s - loss: 564.9013 - val_loss: 442.2797\n",
      "Epoch 56/100\n",
      "12/12 - 0s - loss: 496.6760 - val_loss: 321.9071\n",
      "Epoch 57/100\n",
      "12/12 - 0s - loss: 452.8214 - val_loss: 254.1156\n",
      "Epoch 58/100\n",
      "12/12 - 0s - loss: 439.4127 - val_loss: 231.2050\n",
      "Epoch 59/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Epoch 60/100\n",
      "12/12 - 0s - loss: 445.5275 - val_loss: 254.4183\n",
      "Epoch 61/100\n",
      "12/12 - 0s - loss: 476.0621 - val_loss: 263.8634\n",
      "Epoch 62/100\n",
      "12/12 - 0s - loss: 484.7473 - val_loss: 234.2382\n",
      "Epoch 63/100\n",
      "12/12 - 0s - loss: 458.0667 - val_loss: 231.2050\n",
      "Epoch 64/100\n",
      "12/12 - 0s - loss: 440.2721 - val_loss: 231.2050\n",
      "Epoch 65/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 241.4039\n",
      "Epoch 66/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 262.0634\n",
      "Epoch 67/100\n",
      "12/12 - 0s - loss: 442.1233 - val_loss: 293.4496\n",
      "Epoch 68/100\n",
      "12/12 - 0s - loss: 447.8817 - val_loss: 315.5687\n",
      "Epoch 69/100\n",
      "12/12 - 0s - loss: 451.9398 - val_loss: 335.8766\n",
      "Epoch 70/100\n",
      "12/12 - 0s - loss: 455.6515 - val_loss: 335.5924\n",
      "Epoch 71/100\n",
      "12/12 - 0s - loss: 455.5847 - val_loss: 316.2477\n",
      "Epoch 72/100\n",
      "12/12 - 0s - loss: 452.0642 - val_loss: 298.1476\n",
      "Epoch 73/100\n",
      "12/12 - 0s - loss: 448.7437 - val_loss: 275.2325\n",
      "Epoch 74/100\n",
      "12/12 - 0s - loss: 444.5394 - val_loss: 254.3812\n",
      "Epoch 75/100\n",
      "12/12 - 0s - loss: 439.5161 - val_loss: 239.4852\n",
      "Epoch 76/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Epoch 77/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Epoch 78/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Epoch 79/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Epoch 80/100\n",
      "12/12 - 0s - loss: 442.5522 - val_loss: 231.2050\n",
      "Epoch 81/100\n",
      "12/12 - 0s - loss: 444.6731 - val_loss: 231.2050\n",
      "Epoch 82/100\n",
      "12/12 - 0s - loss: 441.8910 - val_loss: 231.2050\n",
      "Epoch 83/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Epoch 84/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Epoch 85/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Epoch 86/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 233.6415\n",
      "Epoch 87/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 239.5553\n",
      "Epoch 88/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 244.9030\n",
      "Epoch 89/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 249.7377\n",
      "Epoch 90/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 254.1090\n",
      "Epoch 91/100\n",
      "12/12 - 0s - loss: 439.4105 - val_loss: 254.6095\n",
      "Epoch 92/100\n",
      "12/12 - 0s - loss: 439.6046 - val_loss: 251.5875\n",
      "Epoch 93/100\n",
      "12/12 - 0s - loss: 438.4309 - val_loss: 245.3497\n",
      "Epoch 94/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 239.6967\n",
      "Epoch 95/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 234.5731\n",
      "Epoch 96/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Epoch 97/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Epoch 98/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Epoch 99/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Epoch 100/100\n",
      "12/12 - 0s - loss: 438.3087 - val_loss: 231.2050\n",
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = horsepower_model.fit(\n",
    "    y_train, X_train,\n",
    "    epochs=100,\n",
    "    # suppress logging\n",
    "    verbose=2,\n",
    "    # Calculate validation results on 20% of the training data\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>438.308746</td>\n",
       "      <td>231.204956</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>438.308746</td>\n",
       "      <td>231.204956</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>438.308746</td>\n",
       "      <td>231.204956</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>438.308746</td>\n",
       "      <td>231.204956</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>438.308746</td>\n",
       "      <td>231.204956</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss    val_loss  epoch\n",
       "95  438.308746  231.204956     95\n",
       "96  438.308746  231.204956     96\n",
       "97  438.308746  231.204956     97\n",
       "98  438.308746  231.204956     98\n",
       "99  438.308746  231.204956     99"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.ylim([0, 10])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [MPG]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:FaceRec]",
   "language": "python",
   "name": "conda-env-FaceRec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
